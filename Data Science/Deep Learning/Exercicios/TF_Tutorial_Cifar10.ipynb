{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wedeueis/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import prettytensor as pt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/batches.meta\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
      "Size of:\n",
      "- Training-set:\t\t50000\n",
      "- Test-set:\t\t10000\n"
     ]
    }
   ],
   "source": [
    "import cifar10\n",
    "cifar10.maybe_download_and_extract()\n",
    "class_names = cifar10.load_class_names()\n",
    "class_names\n",
    "\n",
    "images_train, cls_train, labels_train = cifar10.load_training_data()\n",
    "images_test, cls_test, labels_test = cifar10.load_test_data()\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(images_train)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(images_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10 import img_size, num_channels, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_plot\n",
    "from helper_plot import plot_images\n",
    "from helper_plot import plot_example_errors\n",
    "from helper_plot import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f942cb478ca5>:3: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "from preprocess import pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_network(images, training):\n",
    "    x_pretty = pt.wrap(images)\n",
    "    if training:\n",
    "        phase = pt.Phase.train\n",
    "    else:\n",
    "        phase = pt.Phase.infer\n",
    "        \n",
    "    with pt.defaults_scope(activation_fn=tf.nn.relu, phase=phase):\n",
    "        y_pred, loss = x_pretty.\\\n",
    "            conv2d(kernel=5, depth=64, name='layer_conv1', batch_normalize=True).\\\n",
    "            max_pool(kernel=2, stride=2).\\\n",
    "            conv2d(kernel=5, depth=64, name='layer_conv2').\\\n",
    "            max_pool(kernel=2, stride=2).\\\n",
    "            flatten().\\\n",
    "            fully_connected(size=256, name='layer_fc1').\\\n",
    "            fully_connected(size=128, name='layer_fc2').\\\n",
    "            softmax_classifier(num_classes=num_classes, labels=y_true)\n",
    "\n",
    "    return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(training):\n",
    "    with tf.variable_scope('network', reuse=not training):\n",
    "        images = x\n",
    "\n",
    "        images = pre_process(images=images, training=training)\n",
    "\n",
    "        y_pred, loss = main_network(images=images, training=training)\n",
    "\n",
    "    return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(initial_value=0,\n",
    "                          name='global_step', trainable=False)\n",
    "\n",
    "_, loss = create_network(training=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, _ = create_network(training=False)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_variable(layer_name):\n",
    "    with tf.variable_scope(\"network/\" + layer_name, reuse=True):\n",
    "        variable = tf.get_variable('weights')\n",
    "\n",
    "    return variable\n",
    "\n",
    "weights_conv1 = get_weights_variable(layer_name='layer_conv1')\n",
    "weights_conv2 = get_weights_variable(layer_name='layer_conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output(layer_name):\n",
    "    tensor_name = \"network/\" + layer_name + \"/Relu:0\"\n",
    "\n",
    "    tensor = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "output_conv1 = get_layer_output(layer_name='layer_conv1')\n",
    "output_conv2 = get_layer_output(layer_name='layer_conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/cifar10_cnn-14000\n",
      "Restored checkpoint from: checkpoints/cifar10_cnn-14000\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "save_path = os.path.join(save_dir, 'cifar10_cnn')\n",
    "\n",
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "    saver.restore(session, save_path=last_chk_path)\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "\n",
    "def random_batch():\n",
    "    num_images = len(images_train)\n",
    "\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=train_batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    x_batch = images_train[idx, :, :, :]\n",
    "    y_batch = labels_train[idx, :]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        x_batch, y_true_batch = random_batch()\n",
    "\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "        i_global, _ = session.run([global_step, optimizer],\n",
    "                                  feed_dict=feed_dict_train)\n",
    "        \n",
    "        if (i_global % 100 == 0) or (i == num_iterations - 1):\n",
    "            batch_acc = session.run(accuracy,\n",
    "                                    feed_dict=feed_dict_train)\n",
    "\n",
    "            msg = \"Global Step: {0:>6}, Training Batch Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(i_global, batch_acc))\n",
    "\n",
    "        if (i_global % 1000 == 0) or (i == num_iterations - 1):\n",
    "            saver.save(session,\n",
    "                       save_path=save_path,\n",
    "                       global_step=global_step)\n",
    "\n",
    "            print(\"Saved checkpoint.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def predict_cls(images, labels, cls_true):\n",
    "    num_images = len(images)\n",
    "\n",
    "    cls_pred = np.zeros(shape=num_images, dtype=np.int)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < num_images:\n",
    "        j = min(i + batch_size, num_images)\n",
    "\n",
    "        feed_dict = {x: images[i:j, :],\n",
    "                     y_true: labels[i:j, :]}\n",
    "\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        i = j\n",
    "\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    return correct, cls_pred\n",
    "\n",
    "def predict_cls_test():\n",
    "    return predict_cls(images = images_test,\n",
    "                       labels = labels_test,\n",
    "                       cls_true = cls_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(correct):\n",
    "    return correct.mean(), correct.sum()\n",
    "\n",
    "def print_test_accuracy():\n",
    "\n",
    "    correct, cls_pred = predict_cls_test()\n",
    "\n",
    "    acc, num_correct = classification_accuracy(correct)\n",
    "\n",
    "    num_images = len(correct)\n",
    "\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, num_correct, num_images))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:  34100, Training Batch Accuracy:  75.0%\n",
      "Global Step:  34200, Training Batch Accuracy:  71.9%\n",
      "Global Step:  34300, Training Batch Accuracy:  65.6%\n",
      "Global Step:  34400, Training Batch Accuracy:  76.6%\n",
      "Global Step:  34500, Training Batch Accuracy:  75.0%\n",
      "Global Step:  34600, Training Batch Accuracy:  75.0%\n",
      "Global Step:  34700, Training Batch Accuracy:  68.8%\n",
      "Global Step:  34800, Training Batch Accuracy:  65.6%\n",
      "Global Step:  34900, Training Batch Accuracy:  78.1%\n",
      "Global Step:  35000, Training Batch Accuracy:  78.1%\n",
      "Saved checkpoint.\n",
      "Time usage: 0:01:56\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 72.1% (7214 / 10000)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
